import torch
import pandas as pd
import os
from torch.utils.data import DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.optim import AdamW
from datasets import load_dataset
from sklearn.metrics import accuracy_score
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter

from config import Config
from kg_enhancer import KGEnhancer
from dataset import KGNewsDataset

# è®¾ç½®å›é€€æœºåˆ¶ï¼Œé˜²æ­¢æŸäº›ä¸å…¼å®¹ MPS çš„ç®—å­å¯¼è‡´æŠ¥é”™
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
config = Config()


def load_and_process_data(split_name, cache_filename):
    """
    é€šç”¨æ•°æ®åŠ è½½ä¸å¤„ç†å‡½æ•°
    :param split_name: huggingface dataset çš„é”®åï¼Œä¾‹å¦‚ 'train' æˆ– 'test'
    :param cache_filename: æœ¬åœ°ä¿å­˜çš„ csv æ–‡ä»¶å
    """
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°ç¼“å­˜
    if os.path.exists(cache_filename):
        print(f"âœ… å‘ç°ç¼“å­˜æ–‡ä»¶: {cache_filename}ï¼Œç›´æ¥è¯»å–ä¸­...")
        return pd.read_csv(cache_filename)

    print(f"ğŸ“¥ æ­£åœ¨ä¸º [{split_name}] é›†ä¸‹è½½/åŠ è½½ AG News æ•°æ®...")
    dataset = load_dataset("ag_news")
    data_split = dataset[split_name]

    # 2. é‡‡æ ·é€»è¾‘ (å¦‚æœ config.SAMPLE_SIZE æœ‰å€¼ï¼Œåˆ™åªå–éƒ¨åˆ†æ•°æ®ç”¨äºè°ƒè¯•)
    if config.SAMPLE_SIZE:
        print(f"âš ï¸ è°ƒè¯•æ¨¡å¼: ä»…ä½¿ç”¨ [{split_name}] çš„å‰ {config.SAMPLE_SIZE} æ¡æ•°æ®ã€‚")
        # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
        select_range = range(min(config.SAMPLE_SIZE, len(data_split)))
        data_split = data_split.select(select_range)

    df = pd.DataFrame(data_split)

    # 3. çŸ¥è¯†å›¾è°±å¢å¼º
    enhancer = KGEnhancer()

    print(f"ğŸ§  å¼€å§‹å¯¹ [{split_name}] é›†è¿›è¡ŒçŸ¥è¯†å›¾è°±å¢å¼º (CPUå¯†é›†å‹)...")
    tqdm.pandas(desc=f"KG Processing ({split_name})")
    df['enhanced_text'] = df['text'].progress_apply(enhancer.enhance_text)

    # 4. ä¿å­˜ç¼“å­˜
    print(f"ğŸ’¾ [{split_name}] å¤„ç†å®Œæˆï¼Œä¿å­˜ç¼“å­˜åˆ° {cache_filename}")
    df.to_csv(cache_filename, index=False)
    return df


def train():
    writer = SummaryWriter(log_dir="runs/agnews_distilbert_kg")

    # å®šä¹‰æ–‡ä»¶å
    train_file = "files/agnews_train_enhanced.csv"
    test_file = "files/agnews_test_enhanced.csv"

    print("=" * 40)
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
    train_df = load_and_process_data(split_name="train", cache_filename=train_file)

    print("=" * 40)
    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    test_df = load_and_process_data(split_name="test", cache_filename=test_file)

    print("=" * 40)
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡:\nè®­ç»ƒé›†å¤§å°: {len(train_df)}\næµ‹è¯•é›†å¤§å°: {len(test_df)}")

    # åˆå§‹åŒ– Tokenizer
    tokenizer = DistilBertTokenizer.from_pretrained(config.MODEL_NAME)

    # æ„å»º Dataset (ç›´æ¥ä½¿ç”¨å…¨é‡æ•°æ®ï¼Œä¸å†è¿›è¡Œ sample/drop åˆ‡åˆ†)
    training_set = KGNewsDataset(train_df, tokenizer, config.MAX_LEN, config)
    # è¿™é‡Œå°† Test é›†ä½œä¸ºéªŒè¯/æµ‹è¯•é›†
    validation_set = KGNewsDataset(test_df, tokenizer, config.MAX_LEN, config)

    # DataLoader é…ç½®
    train_params = {'batch_size': config.BATCH_SIZE, 'shuffle': True, 'num_workers': 0}
    val_params = {'batch_size': config.BATCH_SIZE, 'shuffle': False, 'num_workers': 0}

    training_loader = DataLoader(training_set, **train_params)
    validation_loader = DataLoader(validation_set, **val_params)

    # --- 2. æ¨¡å‹åˆå§‹åŒ– ---
    model = DistilBertForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=4)
    model.to(config.DEVICE)

    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)

    # --- 3. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(config.EPOCHS):
        model.train()
        total_loss = 0
        print(f"\nTraining Epoch {epoch + 1}/{config.EPOCHS}")

        loop = tqdm(training_loader, leave=True)
        for data in loop:
            ids = data['ids'].to(config.DEVICE, dtype=torch.long)
            mask = data['mask'].to(config.DEVICE, dtype=torch.long)
            targets = data['targets'].to(config.DEVICE, dtype=torch.long)

            outputs = model(ids, attention_mask=mask, labels=targets)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            loop.set_description(f"Epoch {epoch + 1}")
            loop.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(training_loader)
        print(f"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}")
        writer.add_scalar("Train/Loss", avg_loss, epoch + 1)

        # --- 4. éªŒè¯/æµ‹è¯•å¾ªç¯ ---
        model.eval()
        val_targets = []
        val_predictions = []

        print("Running Evaluation on Test Set...")
        with torch.no_grad():
            for data in tqdm(validation_loader, desc="Validating"):
                ids = data['ids'].to(config.DEVICE, dtype=torch.long)
                mask = data['mask'].to(config.DEVICE, dtype=torch.long)
                targets = data['targets'].to(config.DEVICE, dtype=torch.long)

                outputs = model(ids, attention_mask=mask)
                _, preds = torch.max(outputs.logits, dim=1)

                val_targets.extend(targets.cpu().numpy())
                val_predictions.extend(preds.cpu().numpy())

        val_acc = accuracy_score(val_targets, val_predictions)
        print(f"ğŸ† Test Set Accuracy: {val_acc:.4f}")
        writer.add_scalar("Test/Accuracy", val_acc, epoch + 1)


        if (epoch + 1) % config.SAVE_CHECKPOINT_INTERVAL == 0:
            if not os.path.exists(config.SAVE_CHECKPOINT_DIR):
                os.makedirs(config.SAVE_CHECKPOINT_DIR, exist_ok=True)

            save_path = os.path.join(config.SAVE_CHECKPOINT_DIR, f"distil-bert_epoch_{epoch + 1}.pth")
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ Checkpoint saved: {save_path}")


if __name__ == "__main__":
    # å¦‚æœä½ æƒ³å…ˆå•ç‹¬ç”Ÿæˆæ•°æ®ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Š
    # load_and_process_data("train", "agnews_train_enhanced.csv")
    # load_and_process_data("test", "agnews_test_enhanced.csv")

    # å¯åŠ¨è®­ç»ƒ (è®­ç»ƒå‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨æ•°æ®åŠ è½½)
    train()